---
title: "Ensemble Model"
author: "Lucia Ronchi Darre"
date: "3/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Load data and predictions from all models

Libraries
```{r}
library(ggplot2)
library(ggrepel)
library(MASS)
library(caret)
```


Save datapath:
```{r}
datapath <- "../../Predictions"
```

Load data:
```{r}
# Train
data.train <- read.csv(paste(datapath, "tiny_sample_train.csv", sep = "/"))
data.train <- data.train[-c(1,2,29)]
# Test
data.test <- read.csv(paste(datapath, "tiny_sample_test.csv", sep = "/"))
data.test <- data.test[-c(1,2,29)]
```

Logistic Regression:
```{r}
# Train
logis.predicted.class.train <- readRDS(paste(datapath, "logis.predicted.tip.train.rds", sep = "/"))
# Test
logis.predicted.class.test <- readRDS(paste(datapath, "logis.predicted.tip.test.rds", sep = "/"))
```

KNN:
```{r}
# Train
knn.predicted.class.train <- read.csv(paste(datapath, "training_result_KNN.csv", sep = "/"))
knn.predicted.class.train <- knn.predicted.class.train["X0"]
# Test
knn.predicted.class.test <- read.csv(paste(datapath, "testing_result_KNN.csv", sep = "/"))
knn.predicted.class.test <- knn.predicted.class.test["X0"]
```

LCA Regression:
```{r}
# Train
lca.predicted.class.train <- readRDS(paste(datapath, "lf_regs.rds", sep = "/"))
lca.predicted.class.train <- lca.predicted.class.train$train_yhat
round(table(lca.predicted.class.train)/length(lca.predicted.class.train),2)
```

Random Forest:
```{r}
# Train
rf.predicted.class.train <- read.csv(paste(datapath, "tree_train.csv", sep = "/"))
rf.predicted.class.train <- rf.predicted.class.train["RandomForest"]
# Test
rf.predicted.class.test <- read.csv(paste(datapath, "tree_test.csv", sep = "/"))
rf.predicted.class.test <- rf.predicted.class.test["RandomForest"]
```

# 2. Checking data consistency

## 2.1 Data

Check that tip_flag is coded as factors with 0s and 1s, with 1s first:
```{r}
print(levels(data.train$tip_flag))
print(levels(data.test$tip_flag))
```

Transform into factors:
```{r}
data.train$tip_flag <- factor(data.train$tip_flag, levels = c(1,0))
data.test$tip_flag <- factor(data.test$tip_flag, levels = c(1,0))
```

Check:
```{r}
print(levels(data.train$tip_flag))
print(levels(data.test$tip_flag))
```

Check that 1 is tip:
```{r}
sum(data.train$tip_flag == 1)/length(data.train$tip_flag)
sum(data.test$tip_flag == 1)/length(data.test$tip_flag)
```

## 2.2 Logistic Regression

Check that Logistic Regression is coded as factors with 0s and 1s, with 1s first:
```{r}
print(levels(logis.predicted.class.train))
print(levels(logis.predicted.class.test))
```

Check that 1 is tip:
```{r}
sum(logis.predicted.class.train == 1)/length(logis.predicted.class.train)
sum(logis.predicted.class.test == 1)/length(logis.predicted.class.test)
```

## 2.3 KNN

Check that KNN is coded as factors with 0s and 1s, with 1s first:
```{r}
print(levels(knn.predicted.class.train))
print(levels(knn.predicted.class.test))
```

Transform into factors:
```{r}
# Train
knn.predicted.class.train <- as.numeric(unlist(knn.predicted.class.train))
knn.predicted.class.train <- factor(knn.predicted.class.train, levels = c(1, 0))
# Test
knn.predicted.class.test <- as.numeric(unlist(knn.predicted.class.test))
knn.predicted.class.test <- factor(knn.predicted.class.test, levels = c(1, 0))
```

Check correct data type:
```{r}
print(levels(knn.predicted.class.train))
print(levels(knn.predicted.class.test))
```

Check that 1 is tip:
```{r}
sum(knn.predicted.class.train == 1)/length(knn.predicted.class.train)
sum(knn.predicted.class.test == 1)/length(knn.predicted.class.test)
```

## 2.4 LCA Regression

Check that LCA is coded as factors with 0s and 1s, with 1s first:
```{r}
print(levels(lca.predicted.class.train))
```

Convert:
```{r}
lca.predicted.class.train <- as.numeric(unlist(lca.predicted.class.train))
lca.predicted.class.train <- factor(lca.predicted.class.train, levels = c(1, 0))
```

Check:
```{r}
print(levels(lca.predicted.class.train))
```

Check that 1 is tip:
```{r}
sum(lca.predicted.class.train == 1)/length(lca.predicted.class.train)
```

## 2.5 Random Forest

Check that RF is coded as factors with 0s and 1s, with 1s first:
```{r}
print(levels(rf.predicted.class.train))
print(levels(rf.predicted.class.test))
```

Transform into factors:
```{r}
# Train
rf.predicted.class.train <- as.numeric(unlist(rf.predicted.class.train)*-1+1)
rf.predicted.class.train <- factor(rf.predicted.class.train, levels = c(1, 0))
# Test
rf.predicted.class.test <- as.numeric(unlist(rf.predicted.class.test)*-1+1)
rf.predicted.class.test <- factor(rf.predicted.class.test, levels = c(1, 0))
```

Check again:
```{r}
print(levels(rf.predicted.class.train))
print(levels(rf.predicted.class.test))
```

Check that 1 is tip:
```{r}
sum(rf.predicted.class.train == 1)/length(rf.predicted.class.train)
sum(rf.predicted.class.test == 1)/length(rf.predicted.class.test)
```


# 3. Building the ensemble Model

Code for ensemble model:
```{r}
# The input for the folliwing model is a list of vectors containing predictions for each model
ensemble.model <- function(x) {
  
  # Get number of models to be compared
  n <- length(x)
  
  # Get number of predictions in each model
  m <- length(x[[1]])
  
  # Store all model predictions as columns of a dataframe
  df <- data.frame(matrix(data = NA, nrow = m, ncol = n))
  for (i in 1:n) {
    df[,i] <- unlist(x[[i]])
  }
  
  # Convert factor levels into numbers for easier comparison across models
  f1 <- levels(x[[1]])[1]
  f2 <- levels(x[[1]])[2]
  df[] <- lapply(df, as.character)
  df[df == f1] <- 1
  df[df == f2] <- 0
  
  # Make each model "vote" for 0s or 1s (voting is done as sum of the rows)
  df["Ensemble"] <- rowSums(sapply(df, as.numeric))

  # Define when to break ties at random
  break.ties <- n/2 # This will be a decimal for odd number of models (so it will never be used, as expected)

  # Make predictions based on majority rule, breaking ties at random
  for (row in 1:m) {
    
    # Set a different seed for each row, to enable random behaviour of the sample below
    set.seed(232323 + row)
    
    # Check whether total votes between models are decisive, or we need to break ties
    if (df[row, "Ensemble"] == break.ties) {
      # Break ties at random by assigning either 0 or 1
      df[row, "Ensemble"] <- sample(x = c(0,1), size = 1)
    } else if (df[row, "Ensemble"] > break.ties) {
      # Majority voted 1
      df[row, "Ensemble"] <- 1
    } else {
      # Majority voted 0
      df[row, "Ensemble"] <- 0
    }
  }  
  
  # Convert numeric predictions into labels again
  df[df$Ensemble == 1, "Ensemble"] <- f1
  df[df$Ensemble == 0, "Ensemble"] <- f2
  df$Ensemble <- factor(df$Ensemble, levels = c(f1, f2))
  
  # Return vector with predicticed classes
  return(df$Ensemble)
}

```


# 4. Predicting Train Values

Predict variable tip_flag based on Logistic Regression, KNN, and Random Forest on Train sample:
```{r}
ensemble.predicted.class.train <- ensemble.model(list(logis.predicted.class.train,
                                                      knn.predicted.class.train,
                                                      rf.predicted.class.train))
```

Confusion matrix for Ensemble model in Train sample:
```{r}
confusionMatrix(data = ensemble.predicted.class.train,
                reference = data.train$tip_flag,
                positive = "0",
                mode = "everything")
```

# 5. Predicting Test Values

Predict variable tip_flag based on Logistic Regression, KNN, and Random Forest on Test sample:
```{r}
ensemble.predicted.class.test <- ensemble.model(list(logis.predicted.class.test,
                                                      knn.predicted.class.test,
                                                      rf.predicted.class.test))
```

Confusion matrix for Ensemble model in Test sample:
```{r}
confusionMatrix(data = ensemble.predicted.class.test,
                reference = data.test$tip_flag,
                positive = "0",
                mode = "everything")
```


# 5. Perfomance Metrics

## 5.1 Train

Store all train results in a list:
```{r}
train.results <- list(logis.predicted.class.train,
                      knn.predicted.class.train,
                      rf.predicted.class.train,
                      ensemble.predicted.class.train,
                      lca.predicted.class.train)
```

Calculate "Accuracy" of each prediction in Train, storing it in another variable:
```{r}
# Initialize empty result vector
accuracy.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in train.results) {
  i <- i + 1
  accuracy.train.models[i] <- confusionMatrix(data = k, 
                                              reference = data.train$tip_flag,
                                              positive = "0")$overall["Accuracy"]
}
```

Calculate "Sensitivity" of each prediction in Train, storing it in another variable:
```{r}
# Initialize empty result vector
sensitivity.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
  i <- i + 1
  sensitivity.train.models[i] <- caret::sensitivity(data = k, 
                                                    reference = data.train$tip_flag,
                                                    positive = "0")
}
```

Calculate "Specificity" of each prediction in Train, storing it in another variable:
```{r}
# Initialize empty result vector
specificity.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
  i <- i + 1
  confusion.matrix.train <- table(data.train$tip_flag, k)
  specificity.train.models[i] <- confusion.matrix.train[1,1]/(confusion.matrix.train[1,1] + confusion.matrix.train[1,2])
}
```

Calculate "Balanced Accuracy" in Train:
```{r}
# Initialize empty result vector
balanced.accuracy.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
  i <- i + 1
  balanced.accuracy.train.models[i] <- (sensitivity.train.models[i] + specificity.train.models[i])/2
}
```


## 5.2 Test

Store all Test results in a list:
```{r}
test.results <- list(logis.predicted.class.test,
                      knn.predicted.class.test,
                      rf.predicted.class.test,
                      ensemble.predicted.class.test)
```

Calculate "Accuracy" of each prediction in Test, storing it in another variable:
```{r}
# Initialize empty result vector
accuracy.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
  i <- i + 1
  accuracy.test.models[i] <- confusionMatrix(data = k, 
                                              reference = data.test$tip_flag,
                                              positive = "0")$overall["Accuracy"]
}
```

Calculate "Sensitivity" of each prediction in Test, storing it in another variable:
```{r}
# Initialize empty result vector
sensitivity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
  i <- i + 1
  sensitivity.test.models[i] <- caret::sensitivity(data = k, 
                                                   reference = data.test$tip_flag,
                                                   positive = "0")
}
```

Calculate "Specificity" of each prediction in Test, storing it in another variable:
```{r}
# Initialize empty result vector
specificity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in test.results) {
  i <- i + 1
  confusion.matrix.test <- table(data.test$tip_flag, k)
  specificity.test.models[i] <- confusion.matrix.test[1,1]/(confusion.matrix.test[1,1] + confusion.matrix.test[1,2])
}
```

Calculate "Balanced Accuracy" in Test:
```{r}
# Initialize empty result vector
balanced.accuracy.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in test.results) {
  i <- i + 1
  balanced.accuracy.test.models[i] <- (sensitivity.test.models[i] + specificity.test.models[i])/2
}
```


# 6. Summary

Generate a dataframe for easier plotting of all results:
```{r}
# Generate vector with model names
models <- factor(c("LR", "KNN", "RF", "Ensemble", "LCA"), 
                 levels = c("LR", "KNN", "RF", "Ensemble", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 4),
                          Set = c(rep("Train", 10), rep("Test", 10)),
                          Metric = c(rep("Accuracy", 5), rep("Balanced Accuracy", 5),
                                     rep("Accuracy", 5), rep("Balanced Accuracy", 5)),
                          Value = c(accuracy.train.models, 
                                    balanced.accuracy.train.models,
                                    c(accuracy.test.models,NA),
                                    c(balanced.accuracy.test.models, NA)))
```

Plot performance metrics by model and type of set (Train vs Test):
```{r}
ggplot(data = performance.metrics,
       aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ensemble", "LCA")), 
           y = Value, 
           group = Set, 
           color = factor(Set, levels = c("Train", "Test")))) +
  geom_point() +
  geom_line() +
  ylim(0, 1) +
  scale_color_manual(values = c("violetred3", "yellowgreen")) +
  facet_grid(~Metric) +
  geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
  labs(title = "Accuracy and Balanced Accuracy by models and set", 
       x = "Model", 
       y = "Performance", 
       color = "Set") +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5), 
        plot.title = element_text(hjust = 0.5),
        legend.position = "bottom")
```




