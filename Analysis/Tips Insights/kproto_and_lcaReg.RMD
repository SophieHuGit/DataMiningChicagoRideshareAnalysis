## Optimize computer processing

```{r Optimize Processing, include=FALSE}
pkgs <- c('doParallel', 'foreach')
lapply(pkgs, require, character.only = T)
cores<-detectCores()
cl <- makeCluster(cores[1]-1)
registerDoParallel(cl)
```

## Load the data and libraries

```{r}
seed = 232323
set.seed(seed)
library(DMwR)
library(clustMixType)
dataPath = "C:/Users/luiseduardo/OneDrive/Documentos/MScA/3. Data Mining/Final Project"
train = read.csv(paste(dataPath,"tiny_sample_train.csv",sep="/"))
test = read.csv(paste(dataPath,"tiny_sample_test.csv",sep="/"))
train = train[,c(3:ncol(train))]
test = test[,c(3:ncol(test))]
```

## Define variables of interest

```{r}
cat_cols = c(8:10,12,16,19,20,21,22,23,25:ncol(train))
train[,cat_cols] = lapply(train[cat_cols], as.factor)
test[,cat_cols] = lapply(test[cat_cols], as.factor)
```


```{r}
summary(train)
```

## Remove problematic data

```{r}
train<-train[!(train$speed_category_dropoff_end=="slow" | train$speed_category_pickup_start=="slow")|
               train$flag_overnight == "1" | train$payment_type == "Other" ,]
test<-test[!(test$speed_category_dropoff_end=="slow" | test$speed_category_pickup_start=="slow")|
             test$flag_overnight == "1" | test$payment_type == "Other" ,]
train$tolls = NULL
test$tolls = NULL
```


## Run K-Means on numeric data (no tips)

```{r}
pred_num_cols = c(2,10,12,14,16,23)
train_num_preds = train[,pred_num_cols]
norm_train_num_preds = scale(train_num_preds)
train_tips = train$tip
VAF_kmeans = list()
for (i in 2:7){
  kmean = kmeans(norm_train_num_preds, centers = i, nstart = 50)
  VAF_kmeans[i] = 1 - kmean$tot.withinss / kmean$totss
}
VAF_kmeans[1] = NULL
```

## Analyze KMean VAFs

```{r}
vaf_df = as.data.frame(VAF_kmeans)
colnames(vaf_df) = c("2","3","4","5","6","7")
plot(x = colnames(vaf_df), y = vaf_df,
     xlab = "Number of groups", ylab = "VAF",
     ylim = c(0,1),
     type = "l")
points(x = colnames(vaf_df), y = vaf_df)
```

## Interpret KMean centers

```{r}
kmean_chosen = kmeans(norm_train_num_preds, centers = 6, nstart = 50, algorithm="MacQueen",iter.max=100)
norm_centers = kmean_chosen$centers
cs = unscale(norm_centers, norm_train_num_preds)
cs
```

More groups, no tips
Cluster 4: High miles   high speed      low bus      expensive -> long trips, likely suburbs
Cluster 6: Medium miles high speed      low bus      medium    -> moving around suburbs
Cluster 1: Low miles    high speed      low bus      cheap     -> moving closely in suburbs
Cluster 3: Low miles    very low speed  very low bus medium    -> going into downtown
Cluster 2: Low miles    low speed       high bus     cheap     -> moving around downtown
Cluster 5: Low miles    low speed       med bus      cheap     -> ??

```{r}
kmean_chosen$size
```

Great majority of "regular trips", but not crushing majority

## Join models with k-prototype

```{r}
p_tot_wss = list()
for (i in 2:8){
  proto = kproto(cbind(norm_train_num_preds,train_cat_preds),
                 k=i,
                 lambda = 1.0,
                 nstart = 5,
                 iter.max = 50,
                 verbose = FALSE)
  p_tot_wss[i] = proto$tot.withinss
}
p_tot_wss[1] = NULL
```

## Choose proto

```{r}
p_tot_wss = as.data.frame(p_tot_wss)
colnames(p_tot_wss) = c("2","3","4","5","6","7","8")
plot(x = colnames(p_tot_wss), y = p_tot_wss, xlab = "Number of groups", ylab = "Total WSS", type = "l")
points(x = colnames(p_tot_wss), y = p_tot_wss)
```

## Analyze chosen proto

```{r}
chosen_proto = kproto(cbind(norm_train_num_preds,train_cat_preds),
                      k=5,
                      lambda = 1.0,
                      nstart = 20,
                      iter.max = 50,
                      verbose = FALSE)
chosen_proto$size
```

```{r}
knn_train_clust = chosen_proto$cluster
train_knn_preds = list()
for (i in 1:max(knn_train_clust)){
  train_knn_preds[i] = sum(train$tip_flag[knn_train_clust == i] == "1")/(sum((train$tip_flag[knn_train_clust == i] == "1"))+sum((train$tip_flag[knn_train_clust == i] == "0")))
}
train_knn_preds
train_knn = as.data.frame(train_knn_preds)
colnames(train_knn) = c("1","2","3","4","5")
plot(x = colnames(train_knn), y = train_knn, ylim = c(0,1), type = "b", ylab = "Proportion of tippers", xlab = "Group", main = "K-Prototype tipper prediction KNN")
```

```{r}
# Note: change categorical variable in ggplots to see different distributions (hour, month, region, etc)
p_c =  chosen_proto$cluster
par(mfrow=c(2,3))
library(ggplot2)
k_data = train[p_c == 1,]
ggplot(k_data, aes(region_pickup)) +
  geom_bar(fill = "blue")
k_data = train[p_c == 2,]
ggplot(k_data, aes(region_pickup)) +
  geom_bar(fill = "blue")
k_data = train[p_c == 3,]
ggplot(k_data, aes(region_pickup)) +
  geom_bar(fill = "blue")
k_data = train[p_c == 4,]
ggplot(k_data, aes(region_pickup)) +
  geom_bar(fill = "blue")
k_data = train[p_c == 5,]
ggplot(k_data, aes(region_pickup)) +
  geom_bar(fill = "blue")
```

## K-Nearest-Neighbors (with best k-measure)

```{r}
kmean_c = kmean_chosen$cluster
kmean_knn_preds = list()
for (i in 1:max(kmean_c)){
  kmean_knn_preds[i] = sum(train$tip_flag[kmean_c == i] == "1")/(sum((train$tip_flag[kmean_c == i] == "1"))+sum((train$tip_flag[kmean_c == i] == "0")))
}
kmean_knn_preds
```

```{r}
proto_c = chosen_proto$cluster
proto_knn_preds = list()
for (i in 1:max(proto_c)){
  proto_knn_preds[i] = sum(train$tip_flag[proto_c == i] == "1")/(sum((train$tip_flag[proto_c == i] == "1"))+sum((train$tip_flag[proto_c == i] == "0")))
}
proto_knn_preds
```



## KNN test validation

```{r}
test_num_preds = test[,pred_num_cols]
norm_test_num_preds = scale(test_num_preds)
test_cat_preds = test[,cat_preds]
test_clustreg = cbind(norm_test_num_preds,test_cat_preds)
proto_knn_test = predict(chosen_proto,newdata=test_clustreg)
knn_test_clust = proto_knn_test$cluster
test_knn_preds = list()
for (i in 1:max(knn_test_clust)){
  test_knn_preds[i] = sum(test$tip_flag[knn_test_clust == i] == "1")/(sum((test$tip_flag[knn_test_clust == i] == "1"))+sum((test$tip_flag[knn_test_clust == i] == "0")))
}
test_knn_preds
test_knn = as.data.frame(test_knn_preds)
colnames(test_knn) = c("1","2","3","4","5")
plot(x = colnames(test_knn), y = test_knn, ylim = c(0,1), type = "b", ylab = "Proportion of tippers", xlab = "Group", main = "K-Prototype tipper prediction KNN")
```

## Prepare data for LCA logistic regression

```{r}
set.seed(232323)
# tip_flag ~ 
clustreg_cols = c(20,2,10,12,23,21,9,7,18)
train_clustreg = train[,clustreg_cols]
# sample_size = floor(0.05*nrow(train_clustreg))
# train_index = sample(seq_len(nrow(train_clustreg)), size = sample_size)
# train_clustreg = train_clustreg[train_index,]
rownames(train_clustreg) = NULL
#ss = round(0.7*sum(train_clustreg["tip_flag"]=="0"),0)
#rm_index = sample(which(train_clustreg["tip_flag"] == "0"), size = ss)
#train_clustreg = train_clustreg[-rm_index,]
train_clustreg = train_clustreg[!train_clustreg[,"payment_type"] == "Other",]
train_clustreg["payment_type"] = ifelse(train_clustreg["payment_type"] == "Mobile","Mobile","Other")
summary(train_clustreg)
```

Note: rebalanced number of tips and no tips for model to run properly

## Flexmix LCA Logistic Regression

```{r}
library(flexmix)
flx = initFlexmix(cbind(as.numeric(train_clustreg$tip_flag)-1,1-as.numeric(train_clustreg$tip_flag)+1) ~ .-1, data = train_clustreg, k = 1:4, model = FLXMRglm(family = "binomial"))
unique(flx)
```

## Choose Flexmix model

```{r}
chosen_flx = flexmix(cbind(as.numeric(train_clustreg$tip_flag)-1,1-as.numeric(train_clustreg$tip_flag)+1) ~ .-1, data = train_clustreg, k = 2, model = FLXMRglm(family = "binomial"))
summary(chosen_flx)
```

## CLR train analysis

```{r}
cls = clusters(chosen_flx)
tp = predict(chosen_flx,train_clustreg, type = "response")
tp = as.data.frame(tp)
train_yhat = c()
train_probs = c()
for(m in 1:nrow(train_clustreg)) {train_probs[m]=tp[m,cls[m]]}
for(m in 1:nrow(train_clustreg)) {train_yhat[m]=ifelse(tp[m,cls[m]]<0.44,0,1)}
table(train_clustreg$tip_flag,train_yhat)
round(prop.table(table(train_clustreg$tip_flag,train_yhat),1),2)
library(AUC)
lca_reg_roc = roc(train_probs,train_clustreg$tip_flag)
plot(lca_reg_roc)
```

## Clusterwise Logistic Regression -> Cross-Validation

```{r}
test_clustreg = test[,clustreg_cols]
test_clustreg = test_clustreg[!test_clustreg[,"payment_type"] == "Other",]
test_clustreg["payment_type"] = ifelse(test_clustreg["payment_type"] == "Mobile","Mobile","Other")

t_cls = clusters(chosen_flx,newdata=train_clustreg)

tp = predict(chosen_flx,train_clustreg, type = "response")
tp = as.data.frame(tp)
train_yhat = c()
head(tp)
for(m in 1:nrow(train_clustreg)) {train_yhat[m]=ifelse(tp[m,t_cls[m]]<0.42,0,1)}
table(test_clustreg$tip_flag,test_yhat)
round(prop.table(table(test_clustreg$tip_flag,test_yhat),1),2)
any(is.na(train_yhat))
```

## Save processed info

``` {r}
res = list(proto_tot_wss = p_tot_wss,
           chosen_proto = chosen_proto,
           proto_knn_preds = proto_knn_preds,
           proto_knn_test = proto_knn_test,
           chosen_flx = chosen_flx,
           lca_reg_roc = lca_reg_roc,
           train_yhat = train_yhat)

saveRDS(res,paste(dataPath,"lf_regs.rds",sep="/"))
```
