i <- i + 1
confusion.matrix.train <- table(data.train$tip_flag, k)
specificity.train.models[i] <- confusion.matrix.train[1,1]/(confusion.matrix.train[1,1] + confusion.matrix.train[1,2])
}
specificity.train.models
test.results <- list(logis.predicted.class.test,
knn.predicted.class.test,
rf.predicted.class.test,
ensemble.predicted.class.test)
# Initialize empty result vector
accuracy.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
i <- i + 1
accuracy.test.models[i] <- confusionMatrix(data = k,
reference = data.test$tip_flag,
positive = "0")$overall["Accuracy"]
}
# Initialize empty result vector
sensitivity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
i <- i + 1
sensitivity.test.models[i] <- caret::sensitivity(data = k,
reference = data.test$tip_flag,
positive = "0")
}
# Initialize empty result vector
specificity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in test.results) {
i <- i + 1
confusion.matrix.test <- table(data.test$tip_flag, k)
specificity.test.models[i] <- confusion.matrix.test[1,1]/(confusion.matrix.test[1,1] + confusion.matrix.test[1,2])
}
specificity.test.models
# Generate vector with model names
models <- factor(c("Log.Reg", "KNN", "R.Forest", "Ensemble", "LCA"),
levels = c("Log.Reg", "KNN", "R.Forest", "Ensemble", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 4),
Set = c(rep("Train", 10), rep("Test", 10)),
Metric = c(rep("Accuracy", 5), rep("Sensitivity", 5),
rep("Accuracy", 5), rep("Sensitivity", 5)),
Value = c(accuracy.train.models, sensitivity.train.models,
c(accuracy.test.models,NA), c(sensitivity.test.models,NA)))
View(performance.metrics)
View(performance.metrics)
# Generate vector with model names
models <- factor(c("Log.Reg", "KNN", "R.Forest", "Ensemble", "LCA"),
levels = c("Log.Reg", "KNN", "R.Forest", "Ensemble", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 6),
Set = c(rep("Train", 15), rep("Test", 15)),
Metric = c(rep("Accuracy", 5), rep("Sensitivity", 5), rep("Specificity", 5),
rep("Accuracy", 5), rep("Sensitivity", 5), rep("Specificity", 5)),
Value = c(accuracy.train.models,
sensitivity.train.models,
specificity.train.models,
c(accuracy.test.models,NA),
c(sensitivity.test.models,NA),
c(specificity.test.models, NA)))
View(performance.metrics)
View(performance.metrics)
head(performance.metrics,10)
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("Log.Reg", "KNN", "R.Forest", "Ensemble", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Sensitivity by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
# Generate vector with model names
models <- factor(c("LR", "KNN", "RF", "Ens", "LCA"),
levels = c("LR", "KNN", "RF", "Ens", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 6),
Set = c(rep("Train", 15), rep("Test", 15)),
Metric = c(rep("Accuracy", 5), rep("Sensitivity", 5), rep("Specificity", 5),
rep("Accuracy", 5), rep("Sensitivity", 5), rep("Specificity", 5)),
Value = c(accuracy.train.models,
sensitivity.train.models,
specificity.train.models,
c(accuracy.test.models,NA),
c(sensitivity.test.models,NA),
c(specificity.test.models, NA)))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Sensitivity by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ggrepel)
library(MASS)
library(caret)
datapath <- "../../Predictions"
# Train
data.train <- read.csv(paste(datapath, "tiny_sample_train.csv", sep = "/"))
data.train <- data.train[-c(1,2,29)]
# Test
data.test <- read.csv(paste(datapath, "tiny_sample_test.csv", sep = "/"))
data.test <- data.test[-c(1,2,29)]
# Train
logis.predicted.class.train <- readRDS(paste(datapath, "logis.predicted.tip.train.rds", sep = "/"))
# Test
logis.predicted.class.test <- readRDS(paste(datapath, "logis.predicted.tip.test.rds", sep = "/"))
# Train
knn.predicted.class.train <- read.csv(paste(datapath, "training_result_KNN.csv", sep = "/"))
knn.predicted.class.train <- knn.predicted.class.train["X0"]
# Test
knn.predicted.class.test <- read.csv(paste(datapath, "testing_result_KNN.csv", sep = "/"))
knn.predicted.class.test <- knn.predicted.class.test["X0"]
# Train
lca.predicted.class.train <- readRDS(paste(datapath, "lf_regs.rds", sep = "/"))
lca.predicted.class.train <- lca.predicted.class.train$train_yhat
round(table(lca.predicted.class.train)/length(lca.predicted.class.train),2)
# Train
rf.predicted.class.train <- read.csv(paste(datapath, "tree_train.csv", sep = "/"))
rf.predicted.class.train <- rf.predicted.class.train["RandomForest"]
# Test
rf.predicted.class.test <- read.csv(paste(datapath, "tree_test.csv", sep = "/"))
rf.predicted.class.test <- rf.predicted.class.test["RandomForest"]
print(levels(data.train$tip_flag))
print(levels(data.test$tip_flag))
data.train$tip_flag <- factor(data.train$tip_flag, levels = c(1,0))
data.test$tip_flag <- factor(data.test$tip_flag, levels = c(1,0))
print(levels(data.train$tip_flag))
print(levels(data.test$tip_flag))
sum(data.train$tip_flag == 1)/length(data.train$tip_flag)
sum(data.test$tip_flag == 1)/length(data.test$tip_flag)
print(levels(logis.predicted.class.train))
print(levels(logis.predicted.class.test))
sum(logis.predicted.class.train == 1)/length(logis.predicted.class.train)
sum(logis.predicted.class.test == 1)/length(logis.predicted.class.test)
print(levels(knn.predicted.class.train))
print(levels(knn.predicted.class.test))
# Train
knn.predicted.class.train <- as.numeric(unlist(knn.predicted.class.train))
knn.predicted.class.train <- factor(knn.predicted.class.train, levels = c(1, 0))
# Test
knn.predicted.class.test <- as.numeric(unlist(knn.predicted.class.test))
knn.predicted.class.test <- factor(knn.predicted.class.test, levels = c(1, 0))
print(levels(knn.predicted.class.train))
print(levels(knn.predicted.class.test))
sum(knn.predicted.class.train == 1)/length(knn.predicted.class.train)
sum(knn.predicted.class.test == 1)/length(knn.predicted.class.test)
print(levels(lca.predicted.class.train))
lca.predicted.class.train <- as.numeric(unlist(lca.predicted.class.train))
lca.predicted.class.train <- factor(lca.predicted.class.train, levels = c(1, 0))
print(levels(lca.predicted.class.train))
sum(lca.predicted.class.train == 1)/length(lca.predicted.class.train)
print(levels(rf.predicted.class.train))
print(levels(rf.predicted.class.test))
# Train
rf.predicted.class.train <- as.numeric(unlist(rf.predicted.class.train)*-1+1)
rf.predicted.class.train <- factor(rf.predicted.class.train, levels = c(1, 0))
# Test
rf.predicted.class.test <- as.numeric(unlist(rf.predicted.class.test)*-1+1)
rf.predicted.class.test <- factor(rf.predicted.class.test, levels = c(1, 0))
print(levels(rf.predicted.class.train))
print(levels(rf.predicted.class.test))
sum(rf.predicted.class.train == 1)/length(rf.predicted.class.train)
sum(rf.predicted.class.test == 1)/length(rf.predicted.class.test)
# The input for the folliwing model is a list of vectors containing predictions for each model
ensemble.model <- function(x) {
# Get number of models to be compared
n <- length(x)
# Get number of predictions in each model
m <- length(x[[1]])
# Store all model predictions as columns of a dataframe
df <- data.frame(matrix(data = NA, nrow = m, ncol = n))
for (i in 1:n) {
df[,i] <- unlist(x[[i]])
}
# Convert factor levels into numbers for easier comparison across models
f1 <- levels(x[[1]])[1]
f2 <- levels(x[[1]])[2]
df[] <- lapply(df, as.character)
df[df == f1] <- 1
df[df == f2] <- 0
# Make each model "vote" for 0s or 1s (voting is done as sum of the rows)
df["Ensemble"] <- rowSums(sapply(df, as.numeric))
# Define when to break ties at random
break.ties <- n/2 # This will be a decimal for odd number of models (so it will never be used, as expected)
# Make predictions based on majority rule, breaking ties at random
for (row in 1:m) {
# Set a different seed for each row, to enable random behaviour of the sample below
set.seed(232323 + row)
# Check whether total votes between models are decisive, or we need to break ties
if (df[row, "Ensemble"] == break.ties) {
# Break ties at random by assigning either 0 or 1
df[row, "Ensemble"] <- sample(x = c(0,1), size = 1)
} else if (df[row, "Ensemble"] > break.ties) {
# Majority voted 1
df[row, "Ensemble"] <- 1
} else {
# Majority voted 0
df[row, "Ensemble"] <- 0
}
}
# Convert numeric predictions into labels again
df[df$Ensemble == 1, "Ensemble"] <- f1
df[df$Ensemble == 0, "Ensemble"] <- f2
df$Ensemble <- factor(df$Ensemble, levels = c(f1, f2))
# Return vector with predicticed classes
return(df$Ensemble)
}
ensemble.predicted.class.train <- ensemble.model(list(logis.predicted.class.train,
knn.predicted.class.train,
rf.predicted.class.train))
confusionMatrix(data = ensemble.predicted.class.train,
reference = data.train$tip_flag,
positive = "0",
mode = "everything")
ensemble.predicted.class.test <- ensemble.model(list(logis.predicted.class.test,
knn.predicted.class.test,
rf.predicted.class.test))
confusionMatrix(data = ensemble.predicted.class.test,
reference = data.test$tip_flag,
positive = "0",
mode = "everything")
train.results <- list(logis.predicted.class.train,
knn.predicted.class.train,
rf.predicted.class.train,
ensemble.predicted.class.train,
lca.predicted.class.train)
# Initialize empty result vector
accuracy.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in train.results) {
i <- i + 1
accuracy.train.models[i] <- confusionMatrix(data = k,
reference = data.train$tip_flag,
positive = "0")$overall["Accuracy"]
}
# Initialize empty result vector
sensitivity.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
i <- i + 1
sensitivity.train.models[i] <- caret::sensitivity(data = k,
reference = data.train$tip_flag,
positive = "0")
}
# Initialize empty result vector
specificity.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
i <- i + 1
confusion.matrix.train <- table(data.train$tip_flag, k)
specificity.train.models[i] <- confusion.matrix.train[1,1]/(confusion.matrix.train[1,1] + confusion.matrix.train[1,2])
}
# Initialize empty result vector
balanced.accuracy.train.models <- rep(NA, 5)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in train.results) {
i <- i + 1
balanced.accuracy.train.models[i] <- (sensitivity.train.models[i] + specificity.train.models[i])/2
}
test.results <- list(logis.predicted.class.test,
knn.predicted.class.test,
rf.predicted.class.test,
ensemble.predicted.class.test)
# Initialize empty result vector
accuracy.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
i <- i + 1
accuracy.test.models[i] <- confusionMatrix(data = k,
reference = data.test$tip_flag,
positive = "0")$overall["Accuracy"]
}
# Initialize empty result vector
sensitivity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting accuracy metric
for (k in test.results) {
i <- i + 1
sensitivity.test.models[i] <- caret::sensitivity(data = k,
reference = data.test$tip_flag,
positive = "0")
}
# Initialize empty result vector
specificity.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in test.results) {
i <- i + 1
confusion.matrix.test <- table(data.test$tip_flag, k)
specificity.test.models[i] <- confusion.matrix.test[1,1]/(confusion.matrix.test[1,1] + confusion.matrix.test[1,2])
}
# Initialize empty result vector
balanced.accuracy.test.models <- rep(NA, 4)
# Initialize iteration variable
i <- 0
# Loop through all results, extracting sensitivity metric
for (k in test.results) {
i <- i + 1
balanced.accuracy.test.models[i] <- (sensitivity.test.models[i] + specificity.test.models[i])/2
}
# Generate vector with model names
models <- factor(c("LR", "KNN", "RF", "Ens", "LCA"),
levels = c("LR", "KNN", "RF", "Ens", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 4),
Set = c(rep("Train", 10), rep("Test", 10)),
Metric = c(rep("Accuracy", 5), rep(" Balanced Accuracy", 5),
rep("Accuracy", 5), rep("Balanced Accuracy", 5)),
Value = c(accuracy.train.models,
balanced.accuracy.train.models,
c(accuracy.test.models,NA),
c(balanced.accuracy.test.models, NA)))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,3)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
# Generate vector with model names
models <- factor(c("LR", "KNN", "RF", "Ens", "LCA"),
levels = c("LR", "KNN", "RF", "Ens", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 4),
Set = c(rep("Train", 10), rep("Test", 10)),
Metric = c(rep("Accuracy", 5), rep("Balanced Accuracy", 5),
rep("Accuracy", 5), rep("Balanced Accuracy", 5)),
Value = c(accuracy.train.models,
balanced.accuracy.train.models,
c(accuracy.test.models,NA),
c(balanced.accuracy.test.models, NA)))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,3)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,4)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
ylim(0, 1) +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,4)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
ylim(0, 1) +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,4)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5),
legend.position = "bottom")
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ens", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
ylim(0, 1) +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5),
legend.position = "bottom")
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ensemble", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
ylim(0, 1) +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5),
legend.position = "bottom")
# Generate vector with model names
models <- factor(c("LR", "KNN", "RF", "Ensemble", "LCA"),
levels = c("LR", "KNN", "RF", "Ensemble", "LCA"))
# Create dataframe to compile results
performance.metrics <- data.frame(Model = rep(models, 4),
Set = c(rep("Train", 10), rep("Test", 10)),
Metric = c(rep("Accuracy", 5), rep("Balanced Accuracy", 5),
rep("Accuracy", 5), rep("Balanced Accuracy", 5)),
Value = c(accuracy.train.models,
balanced.accuracy.train.models,
c(accuracy.test.models,NA),
c(balanced.accuracy.test.models, NA)))
ggplot(data = performance.metrics,
aes(x = factor(Model, levels = c("LR", "KNN", "RF", "Ensemble", "LCA")),
y = Value,
group = Set,
color = factor(Set, levels = c("Train", "Test")))) +
geom_point() +
geom_line() +
ylim(0, 1) +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
facet_grid(~Metric) +
geom_text_repel(aes(label = round(Value,2)), size = 3, nudge_y = 0.01) +
labs(title = "Accuracy and Balanced Accuracy by models and set",
x = "Model",
y = "Performance",
color = "Set") +
theme(axis.text.x = element_text(angle = 0, hjust = 0.5),
plot.title = element_text(hjust = 0.5),
legend.position = "bottom")
