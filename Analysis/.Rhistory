k2.model.train$results[[2]]$coefficients
k2.model.train$results[[1]]$coefficients
k2.model.train$results[[2]]$coefficients
k2.model.train$results
k2.model.train$results[[1]]$coefficients
k2.model.train$results[[2]][[1]]$coefficients
rbind(k2.model.train$results[[1]]$coefficients,
k2.model.train$results[[2]][[1]]$coefficients)
rbind(cbind(k2.model.train$results[[1]]$coefficients, Cluster = rep(1,7)),
cbind(k2.model.train$results[[2]][[1]]$coefficients, Cluster = rep(2,7)))
rbind(cbind(k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(cbind(Coefficient = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Coefficient = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients),2),
cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(Coefficient = rownames(k2.model.train$results[[1]]$coefficients)*2,
cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rownames(k2.model.train$results[[1]]$coefficients)
rep(rownames(k2.model.train$results[[1]]$coefficients),2)
rbind(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients),2),
cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(cbind(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients),2)),
cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1], Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1], Cluster = rep(2,7)))
rbind(cbind(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients),
Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients),
Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))))
rbind(cbind(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- rep(rownames(k2.coefficients), 2)
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- rep(rownames(k2.coefficients), 2)
k2.coefficients
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- rownames(k2.coefficients)
k2.coefficients
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- rownames(k2.coefficients)
k2.coefficients
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients
rep(rownames(k2.coefficients), 2)
k2.coefficients["Coefficient"] <- rep(rownames(k2.coefficients), 2)
k2.coefficients
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- c(rownames(k2.coefficients), rownames(k2.coefficients))
k2.coefficients
nrow(k2.model.train$results[[1]]$coefficients)
nrow(k2.model.train$results[[2]][[1]]$coefficients)
k2.coefficients <- rbind(cbind(Estimate = k2.model.train$results[[1]]$coefficients[,1],
Cluster = rep(1,7)),
cbind(Estimate = k2.model.train$results[[2]][[1]]$coefficients[,1],
Cluster = rep(2,7)))
k2.coefficients["Coefficient"] <- c("a", "a", "a", "a", "a", "a", "a","a","a","a","a","a","a","a")
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rep(rownames(k2.model.train$results[[1]]$coefficients), 2),
Estimate = c(k2.model.train$results[[1]]$coefficients[,1],
k2.model.train$results[[2]][[1]]$coefficients[,1]),
Cluster = c(rep(1, 7), rep(2, 7)))
k2.coefficients
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, color = Cluster)) +
geom_bar()
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, color = Cluster)) +
geom_bar(stat = "identity")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "dodge")
ggplot(data = k2.coefficients,
aes(x = Cluster, y = Estimate, fill = Coefficient)) +
geom_bar(stat = "identity", position = "dodge")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "dodge")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(position = "dodge")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "dodge")
ggplot(data = k2.coefficients,
aes(x = Coefficient, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "fill")
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
k2.coefficients["Sign"] <- sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2)
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
k2.coefficients["Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
rownames(k2.coefficients) <- NULL
k2.coefficients["Change.Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
rownames(k2.coefficients) <- NULL
k2.coefficients["Change.Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients["Magnitude.Diff"] <- k2.coefficients$Estimate.1/k2.coefficients$Estimate.2
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
rownames(k2.coefficients) <- NULL
k2.coefficients["Change.Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients["Magnitude.Diff"] <- round(k2.coefficients$Estimate.1/k2.coefficients$Estimate.2,1)
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = k2.model.train$results[[1]]$coefficients[,1],
Estimate.2 = k2.model.train$results[[2]][[1]]$coefficients[,1])
rownames(k2.coefficients) <- NULL
k2.coefficients["Change.Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients["Magnitude.Diff"] <- round(k2.coefficients$Estimate.1/k2.coefficients$Estimate.2)
k2.coefficients
k2.coefficients <- data.frame(Coefficient = rownames(k2.model.train$results[[1]]$coefficients),
Estimate.1 = round(k2.model.train$results[[1]]$coefficients[,1],2),
Estimate.2 = round(k2.model.train$results[[2]][[1]]$coefficients[,1],2))
rownames(k2.coefficients) <- NULL
k2.coefficients["Change.Sign"] <- (sign(k2.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
k2.coefficients["Magnitude.Diff"] <- round(k2.coefficients$Estimate.1/k2.coefficients$Estimate.2)
k2.coefficients
k1.model.train$results
k1.model.train$results[[1]]
k2.model.train$results[[1]]
k2.model.train$results
k3.model.train$results
k3.model.train$results[[1]]$coefficients[,1]
k3.model.train$results[[2]][[1]]$coefficients[,1]
k3.model.train$results[[3]][[1]]$coefficients[,1]
k3.model.train$results
k3.model.train$results[[2]][[2]][[1]]$coefficients[,1]
k3.coefficients <- data.frame(Coefficient = rownames(k3.model.train$results[[1]]$coefficients),
Estimate.1 = round(k3.model.train$results[[1]]$coefficients[,1], 2),
Estimate.2 = round(k3.model.train$results[[2]][[1]]$coefficients[,1], 2),
Estimate.3 = round(k3.model.train$results[[2]][[2]][[1]]$coefficients[,1], 2))
rownames(k3.coefficients) <- NULL
#k3.coefficients["Change.Sign"] <- (sign(k3.coefficients$Estimate.1*k2.coefficients$Estimate.2) == -1)
#k3.coefficients["Magnitude.Diff"] <- round(k3.coefficients$Estimate.1/k2.coefficients$Estimate.2)
#k3.coefficients
k3.coefficients
k3.coefficients.for.plot <- data.frame(Coefficient = rep(k3.coefficients$Coefficient, 3),
Estimate = c(k3.coefficients$Estimate.1,
k3.coefficients$Estimate.2,
k3.coefficients$Estimate.3),
Cluster = c(rep(1,7), rep(2,7), rep(3,7)))
k3.coefficients.for.plot
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate)) +
facet_wrap(~Coefficient)
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate)) +
geom_bar(stat = "identity")
facet_wrap(~Coefficient)
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate)) +
geom_bar()
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate)) +
geom_bar(stat = "identity", position = "dodge")
facet_wrap(~Coefficient)
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "dodge")
facet_wrap(~Coefficient)
ggplot(data = k3.coefficients.for.plot,
aes(x = Cluster, y = Estimate, fill = Cluster)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~Coefficient)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(MASS)
datapath <- "/Users/luciaronchi/Documents/MSCA/Data_Mining_Principles/Assignments/Assignment04/Logistic_Regression"
GC.train <- readRDS(paste(datapath, "train_sample.rds", sep = "/"))
GC.test <- readRDS(paste(datapath, "test_sample.rds", sep = "/"))
lda(formula = Class ~ .,
data = GC.train)
lda.train <- lda(formula = Class ~ ., data = GC.train)
qda.train <- qda(formula = Class ~ ., data = GC.train)
qda.train <- qda(formula = Class ~ ., data = GC.train)
colnames(GC.train)
predict(lda.train, newdata = GC.test)$class
NumberPeopleMaintenance + Telephone + ForeignWorker + CheckingAccountStatus + CreditHistory"
qda.train <- qda(formula = fla, data = GC.train)
predict(qda.train, newdata = GC.test)$class
predict(qda.train, newdata = GC.test)$class
(predict(qda.train, newdata = GC.test)$class)
p <- predict(qda.train, newdata = GC.test)$class
p
p <- predict(qda.train, newdata = GC.test)$class
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(MASS)
library(ggplot2)
library(ggrepel)
library(gains)
library(AUC)
data <- read.csv("tiny_sample.csv")
data <- read.csv("tiny_sample.csv")
setwd("~/Documents/MSCA/Data_Mining_Principles/Final_Project/DataMiningChicagoRideshareAnalysis/Analysis")
data <- read.csv("tiny_sample.csv")
data <- read.csv("../../tiny_sample.csv")
data$X <- NULL
data$outlier <- NULL
str(data)
sum(data[data$tip_flag == 1, "tip_flag"])/nrow(data)
data[data$tip_flag == 1, "tip_flag"] <- "Yes"
data[data$tip_flag == 0, "tip_flag"] <- "No"
data$tip_flag <- factor(data$tip_flag, levels = c("Yes", "No"))
data$flag_overnight <- factor(data$flag_overnight, levels = c(0,1), labels = c("No", "Yes"))
data$flag_weekend  <- factor(data$flag_weekend , levels = c(0,1), labels = c("No", "Yes"))
sum(data$tip_flag == "Yes")/nrow(data)
levels(data$payment_type) <- c("Mobile", "Card", "Cash")
str(data)
# Set seed
set.seed(232323)
# Set train set size (70%)
sample.size <- floor(0.70*nrow(data))
# Create indeces of rows to sample
sample.indeces <- sample.int(n = nrow(data), size = sample.size)
# Create train and test sets according to these sample indices
data.train <- data[sample.indeces, ]
data.test <- data[-sample.indeces, ]
reduced.model <- glm(formula = tip_flag ~ duration_seconds + miles + payment_type + speed_dropoff_end
+ ride_type + total_no_tip,
family = binomial(link=logit),
data = data.train)
reduced.model <- glm(formula = tip_flag ~ duration_seconds + miles + payment_type + speed_dropoff_end
+ ride_type + total_no_tip,
family = binomial(link=logit),
data = data.train)
AIC(reduced.model)
summary(reduced.model)
round(sum(data.train$tip_flag == "No")/nrow(data.train),2)
(gains.train <- gains(actual = as.numeric(data.train$tip_flag) - 1,
predicted = reduced.model$fitted.values,
groups = 20))
predicted.class.train <- predict(object = reduced.model, type = c("response"))
predicted.class.train[predicted.class.train >= 0.8] <- "No"
predicted.class.train[predicted.class.train < 0.8] <- "Yes"
predicted.class.train <- factor(predicted.class.train, levels = c("Yes", "No"))
confusionMatrix(data = predicted.class.train,
reference = data.train$tip_flag,
positive = "No",
mode = "everything")
plot(gains.train)
# Construct ROC curve
ROC.train <- roc(reduced.model$fitted.values, factor(as.numeric(data.train$tip_flag)-1))
# Get area under the curve
AUC.train <- auc(ROC.train)
plot(ROC.train, main = paste("ROC Curve for Train Set", "\n",
"AUC: ", round(AUC.train, 4)))
names(ROC.train)
ggplot(data = data.frame(cutoffs = ROC.train$cutoffs,
TPR = ROC.train$tpr,
FPR = ROC.train$fpr),
aes(x = FPR)) +
geom_line(aes(y = TPR, color = "TPR")) +
geom_abline(slope=1, intercept=0, linetype = "dashed", color = "grey") +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
labs(title = "ROC Curve",
x = "FPR",
y = "TPR") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text.x = element_text(angle = 0, hjust = 1),
plot.title = element_text(hjust = 0.5))
# Generate iteration variable
i <- 10
# Generate vectors to store results
n <- length(data.train)
predicted.class <- rep(NA, n)
TPR <- rep(NA, i-1)
TNR <- rep(NA, i-1)
# Iterate with different thresholds
for (t in seq(1/i, 1-1/i, 1/i)) {
# Predict values
predicted.class <- reduced.model$fitted.values
predicted.class[predicted.class >= t] <- "No"
predicted.class[predicted.class < t] <- "Yes"
predicted.class <- factor(predicted.class, levels = c("Yes", "No"))
# Generate confusion matrix
(confusion.matrix.train <- table(data.train$tip_flag, predicted.class))
# True positive rate (sensitivity): TP / (TP + FN)
TPR[t*i] <- confusion.matrix.train[2,2]/(confusion.matrix.train[2,2] + confusion.matrix.train[2,1])
# True negative rate (specificity): TN / (TN + FP)
TNR[t*i] <- confusion.matrix.train[1,1]/(confusion.matrix.train[1,1] + confusion.matrix.train[1,2])
}
ggplot(data = data.frame(Decision.Boundary = seq(1/i, 1-1/i, 1/i),
TPR = round(TPR, 2),
TNR = round(TNR, 2)),
aes(x = Decision.Boundary)) +
geom_line(aes(y = TPR, color = "Sensitivity")) +
geom_line(aes(y = TNR, color = "Specificity")) +
geom_text_repel(aes(y = TNR, label = TNR), size = 2.5, nudge_y = -0.025) +
geom_text_repel(aes(y = TPR, label = TPR), size = 2.5, nudge_y = 0.025) +
geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey") +
geom_vline(xintercept = 0.8, linetype = "dashed", color = "grey") +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
labs(title = "Sensitivity vs Specificity for Train dataset",
x = "Decision Boundary",
y = "Rate (%)",
color = "Metric") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text.x = element_text(angle = 0, hjust = 1),
plot.title = element_text(hjust = 0.5))
predicted.class.test <- predict(object = reduced.model, newdata = data.test, type = c("response"))
predicted.class.test[predicted.class.test >= 0.8] <- "No"
predicted.class.test[predicted.class.test < 0.8] <- "Yes"
predicted.class.test <- factor(predicted.class.test, levels = c("Yes", "No"))
confusionMatrix(data = predicted.class.test, reference = data.test$tip_flag,
positive = "No", mode = "everything")
confusionMatrix(data = predicted.class.train, reference = data.train$tip_flag,
positive = "No", mode = "everything")
predicted.class
predicted.class(predicted.class == "Yes") <- 1
predicted.class[predicted.class == "Yes"] <- 1
predicted.class[predicted.class == "No"] <- 1
predicted.class <- as.character(predicted.class)
predicted.class <- as.character(predicted.class)
predicted.class[predicted.class == "Yes"] <- 1
predicted.class[predicted.class == "No"] <- 1
predicted.class
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(caret)
library(MASS)
library(ggplot2)
library(ggrepel)
library(gains)
library(AUC)
data <- read.csv("../../tiny_sample.csv")
data$X <- NULL
data$outlier <- NULL
str(data)
sum(data[data$tip_flag == 1, "tip_flag"])/nrow(data)
data[data$tip_flag == 1, "tip_flag"] <- "Yes"
data[data$tip_flag == 0, "tip_flag"] <- "No"
data$tip_flag <- factor(data$tip_flag, levels = c("Yes", "No"))
data$flag_overnight <- factor(data$flag_overnight, levels = c(0,1), labels = c("No", "Yes"))
data$flag_weekend  <- factor(data$flag_weekend , levels = c(0,1), labels = c("No", "Yes"))
sum(data$tip_flag == "Yes")/nrow(data)
levels(data$payment_type) <- c("Mobile", "Card", "Cash")
str(data)
# Set seed
set.seed(232323)
# Set train set size (70%)
sample.size <- floor(0.70*nrow(data))
# Create indeces of rows to sample
sample.indeces <- sample.int(n = nrow(data), size = sample.size)
# Create train and test sets according to these sample indices
data.train <- data[sample.indeces, ]
data.test <- data[-sample.indeces, ]
reduced.model <- glm(formula = tip_flag ~ duration_seconds + miles + payment_type + speed_dropoff_end
+ ride_type + total_no_tip,
family = binomial(link=logit),
data = data.train)
AIC(reduced.model)
summary(reduced.model)
round(sum(data.train$tip_flag == "No")/nrow(data.train),2)
(gains.train <- gains(actual = as.numeric(data.train$tip_flag) - 1,
predicted = reduced.model$fitted.values,
groups = 20))
predicted.class.train <- predict(object = reduced.model, type = c("response"))
predicted.class.train[predicted.class.train >= 0.8] <- "No"
predicted.class.train[predicted.class.train < 0.8] <- "Yes"
predicted.class.train <- factor(predicted.class.train, levels = c("Yes", "No"))
confusionMatrix(data = predicted.class.train,
reference = data.train$tip_flag,
positive = "No",
mode = "everything")
plot(gains.train)
# Construct ROC curve
ROC.train <- roc(reduced.model$fitted.values, factor(as.numeric(data.train$tip_flag)-1))
# Get area under the curve
AUC.train <- auc(ROC.train)
plot(ROC.train, main = paste("ROC Curve for Train Set", "\n",
"AUC: ", round(AUC.train, 4)))
names(ROC.train)
ggplot(data = data.frame(cutoffs = ROC.train$cutoffs,
TPR = ROC.train$tpr,
FPR = ROC.train$fpr),
aes(x = FPR)) +
geom_line(aes(y = TPR, color = "TPR")) +
geom_abline(slope=1, intercept=0, linetype = "dashed", color = "grey") +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
labs(title = "ROC Curve",
x = "FPR",
y = "TPR") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text.x = element_text(angle = 0, hjust = 1),
plot.title = element_text(hjust = 0.5))
# Generate iteration variable
i <- 10
# Generate vectors to store results
n <- length(data.train)
predicted.class <- rep(NA, n)
TPR <- rep(NA, i-1)
TNR <- rep(NA, i-1)
# Iterate with different thresholds
for (t in seq(1/i, 1-1/i, 1/i)) {
# Predict values
predicted.class <- reduced.model$fitted.values
predicted.class[predicted.class >= t] <- "No"
predicted.class[predicted.class < t] <- "Yes"
predicted.class <- factor(predicted.class, levels = c("Yes", "No"))
# Generate confusion matrix
(confusion.matrix.train <- table(data.train$tip_flag, predicted.class))
# True positive rate (sensitivity): TP / (TP + FN)
TPR[t*i] <- confusion.matrix.train[2,2]/(confusion.matrix.train[2,2] + confusion.matrix.train[2,1])
# True negative rate (specificity): TN / (TN + FP)
TNR[t*i] <- confusion.matrix.train[1,1]/(confusion.matrix.train[1,1] + confusion.matrix.train[1,2])
}
ggplot(data = data.frame(Decision.Boundary = seq(1/i, 1-1/i, 1/i),
TPR = round(TPR, 2),
TNR = round(TNR, 2)),
aes(x = Decision.Boundary)) +
geom_line(aes(y = TPR, color = "Sensitivity")) +
geom_line(aes(y = TNR, color = "Specificity")) +
geom_text_repel(aes(y = TNR, label = TNR), size = 2.5, nudge_y = -0.025) +
geom_text_repel(aes(y = TPR, label = TPR), size = 2.5, nudge_y = 0.025) +
geom_vline(xintercept = 0.5, linetype = "dashed", color = "grey") +
geom_vline(xintercept = 0.8, linetype = "dashed", color = "grey") +
scale_color_manual(values = c("violetred3", "yellowgreen")) +
labs(title = "Sensitivity vs Specificity for Train dataset",
x = "Decision Boundary",
y = "Rate (%)",
color = "Metric") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text.x = element_text(angle = 0, hjust = 1),
plot.title = element_text(hjust = 0.5))
predicted.class.test <- predict(object = reduced.model, newdata = data.test, type = c("response"))
predicted.class.test[predicted.class.test >= 0.8] <- "No"
predicted.class.test[predicted.class.test < 0.8] <- "Yes"
predicted.class.test <- factor(predicted.class.test, levels = c("Yes", "No"))
confusionMatrix(data = predicted.class.test, reference = data.test$tip_flag,
positive = "No", mode = "everything")
confusionMatrix(data = predicted.class.train, reference = data.train$tip_flag,
positive = "No", mode = "everything")
predicted.class <- as.character(predicted.class)
predicted.class[predicted.class == "Yes"] <- 1
predicted.class[predicted.class == "No"] <- 1
predicted.class
saveRDS(object = predicted.class, "logis.predicted.")
levels(predicted.class.train)
levels(predicted.class.train) <- c(1, 0)
predicted.class.train
sum(predicted.class.train)/length(predicted.class.train)
sum(predicted.class.train == 1)/length(predicted.class.train)
sum(predicted.class.train == 0)/length(predicted.class.train)
levels(predicted.class.test) <- c(1, 0)
sum(predicted.class.test == 0)/length(predicted.class.test)
saveRDS(object = predicted.class.train, "logis.predicted.tip.train.rds")
saveRDS(object = predicted.class.test, "logis.predicted.tip.test.rds")
