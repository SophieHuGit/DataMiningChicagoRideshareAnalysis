---
title: "DM-Project-PCA"
author: "MiaoxuanZhang"
date: "2/18/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(readr)
project_data <- read_csv("~/Desktop/Data Mining/Project/project_data.csv")
head(project_data)

```
Now let us select all maybe related quantitative columns
```{r}
quant_data=project_data[,c("duration_seconds","miles","tip",fare","tolls","extra_charges",
                          "trip_total","speed_pickup_start","bus_count_pickup_start","gps_pings_pickup_start","speed_dropoff_end","bus_count_dropoff_end","gps_pings_dropoff_end")]
head(quant_data)
```
### PCA
Now let us convert each element in this list from "list" to "numeric"
```{r}
quant_data <- data.matrix(quant_data)
help(prcomp)
quant_data<- scale(quant_data)
quant_data[is.nan(quant_data)] <- 0
PCA.result=prcomp(na.omit(quant_data))
```

Now let us check Scree plot
```{r}
x=1:12
y=cumsum((PCA.result$sdev)^2/sum((PCA.result$sdev)^2))
plot(x,y, type="b", col="blue", xlab="Number of components", ylab="Variance account for")
text(y, labels = round(y,2), pos = 4)
```
Elbow point is when number of components=8

So I want to keep 8 components
```{r}
loadings=PCA.result$rotation
loadings
```
Rotate the component loadings using varimax rotation.
```{r}
yRotate=varimax(PCA.result$rotation[,1:7])
yRotate
```
We can see from above PC1 is mostly pickup traffic. PC2 is mostly fare related. PC3 is mostly trip distance (negative). PC4 is speed (negative). PC5 is tolls. PC6 is fare related (extra charge) as well. PC7 is mostly dropoff traffic and start speed, and PC8 is tips. 

```{r}
PCA_data=PCA.result$x[,1:8]
```

```{r}
colnames(PCA_data)=c("pickup_traffic","trip_fare","trip_distance","speed","tolls","extra charge","pick_up speed and drop_off traffic","tips")
PCA_data=as.data.frame(PCA_data)
head(PCA_data)


```

### Linear regression
Very bad result
```{r}
PCA.lm=lm(tips~., data=PCA_data)
summary(PCA.lm) 
```


```{r}
library(MASS)
stepAIC(PCA.lm, direction="both")
```

##Let us now try logistic regression on PCA result ???
```{r}
#Change tips to 1 if tips>0 and tips=0 if tips=0
boolean=quant_data[,"tip"]>0

quant_data[,"tip"][boolean]=1

quant_data[,"tip"][!boolean]=0
quant_data=na.exclude(quant_data)

quant_data=as.data.frame(quant_data) #Exclude NAs
quant_data[,"tip"]=as.factor(quant_data[,"tip"])
dim(quant_data)
head(quant_data)
```

Let us select the model with lowest AIC
```{r}
glm(tip~., data=quant_data, family=binomial(link=logit))

```

```{r}
library(MASS)
stepAIC(glm(tip~., data=quant_data, family=binomial(link=logit)), direction="both", maxit=100)
```

### Tree model
```{r}
require(rpart)
set.seed("1000001")

x=rpart(tip~.,control=rpart.control(cp=0,minsplit=30,xval=10, maxsurrogate=0), data=quant_data)
par(mai=c(0.1,0.1,0.1,0.1)) #set plot margin
plot(x,main="Classification Tree: German Credit Train Data",col=3, compress=TRUE, branch=0.2,uniform=TRUE)
text(x,cex=0.6,col=4,use.n=TRUE,fancy=TRUE,fwidth=0.3,fheight=0.3,bg=c(5))
```
Prune the tree
```{r}
library(rpart.plot)
library(RColorBrewer)
library(rattle)
ptree=prune(x, cp=x$cptable[which.min(x$cptable[,"xerror"]),"CP"])
fancyRpartPlot(ptree, uniform=TRUE,main="Pruned Classification Tree")
```







